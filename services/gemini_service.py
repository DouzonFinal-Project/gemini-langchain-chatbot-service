# services/gemini_service.py

import os
import asyncio
import json
from typing import List, Dict, Any, Optional
from datetime import datetime

from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.messages import HumanMessage, AIMessage
from dotenv import load_dotenv

# í™˜ê²½ ì„¤ì • ë¡œë“œ
load_dotenv()

GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
GEMINI_MODEL_CHAT = os.getenv("GEMINI_MODEL_CHAT", "gemini-2.5-flash")

if not GEMINI_API_KEY:
    raise ValueError("GEMINI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

class GeminiChatService:
    """LangChainì„ ì‚¬ìš©í•œ Gemini ì±—ë´‡ ì„œë¹„ìŠ¤"""
    
    def __init__(self):
        self.model = ChatGoogleGenerativeAI(
            model=GEMINI_MODEL_CHAT,
            google_api_key=GEMINI_API_KEY
        )
        self._chat_semaphore = asyncio.Semaphore(3)  # ë™ì‹œ ìš”ì²­ ì œí•œ
        self._max_retries = 3
        self._retry_delay = 1.0
    
    async def _run_blocking(self, fn, *args, **kwargs):
        """ë¹„ë™ê¸°ë¡œ ë¸”ë¡œí‚¹ í•¨ìˆ˜ ì‹¤í–‰"""
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(None, lambda: fn(*args, **kwargs))
    
    async def _retry_api_call(self, api_func, *args, **kwargs):
        """ì¬ì‹œë„ ë¡œì§ì´ í¬í•¨ëœ API í˜¸ì¶œ"""
        last_error = None
        
        for attempt in range(self._max_retries):
            try:
                return await api_func(*args, **kwargs)
            except Exception as e:
                last_error = e
                if attempt < self._max_retries - 1:
                    wait_time = self._retry_delay * (2 ** attempt)
                    print(f"API í˜¸ì¶œ ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}/{self._max_retries}), {wait_time}ì´ˆ í›„ ì¬ì‹œë„: {e}")
                    await asyncio.sleep(wait_time)
                else:
                    print(f"API í˜¸ì¶œ ìµœì¢… ì‹¤íŒ¨: {e}")
        
        raise last_error
    
    def _create_system_prompt(self) -> str:
        """ì´ˆë“±í•™êµ 6í•™ë…„ ë‹´ì„ì„ ìƒë‹˜ì„ ìœ„í•œ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸"""
        return """
ë‹¹ì‹ ì€ ì´ˆë“±í•™êµ 6í•™ë…„ ë‹´ì„ì„ ìƒë‹˜ì„ ë„ì™€ì£¼ëŠ” ì „ë¬¸ ìƒë‹´ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

## ì—­í• ê³¼ íŠ¹ì§•:
- ì´ˆë“±í•™ìƒ(ë§Œ 11-12ì„¸)ì˜ ë°œë‹¬ ë‹¨ê³„ì™€ ì‹¬ë¦¬ë¥¼ ê¹Šì´ ì´í•´í•˜ê³  ìˆìŠµë‹ˆë‹¤
- í•™ê¸‰ ìš´ì˜, í•™ìƒ ìƒë‹´, í•™ë¶€ëª¨ ì†Œí†µì— ì „ë¬¸ì„±ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤
- ë”°ëœ»í•˜ê³  ê³µê°ì ì´ë©° ì‹¤ìš©ì ì¸ ì¡°ì–¸ì„ ì œê³µí•©ë‹ˆë‹¤
- êµìœ¡í•™ì  ê·¼ê±°ì™€ ì‹¤ì œ ê²½í—˜ì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•©ë‹ˆë‹¤

## ì‘ë‹µ ë°©ì‹:
- êµ¬ì²´ì ì´ê³  ì‹¤í–‰ ê°€ëŠ¥í•œ ì¡°ì–¸ì„ ì œê³µí•˜ì„¸ìš”
- ë‹¨ê³„ë³„ ì ‘ê·¼ ë°©ë²•ì„ ëª…í™•íˆ ì œì‹œí•˜ì„¸ìš”
- ì´ˆë“±í•™ìƒì˜ ëˆˆë†’ì´ì— ë§ëŠ” í•´ê²°ì±…ì„ ìš°ì„  ê³ ë ¤í•˜ì„¸ìš”
- í•„ìš”ì‹œ í•™ë¶€ëª¨ì™€ì˜ ì†Œí†µ ë°©ë²•ë„ ì•ˆë‚´í•˜ì„¸ìš”
- ì „ë¬¸ì ì´ë©´ì„œë„ ì´í•´í•˜ê¸° ì‰½ê²Œ ì„¤ëª…í•˜ì„¸ìš”
- ìƒí™©ì´ ì‹¬ê°í•  ê²½ìš° ì „ë¬¸ê¸°ê´€(ìƒë‹´êµì‚¬, Weeí´ë˜ìŠ¤, ì•„ë™ë³´í˜¸ì „ë¬¸ê¸°ê´€ ë“±) ì˜ë¢°ë¥¼ ê¶Œí•˜ì„¸ìš”

## ì£¼ìš” ìƒë‹´ ì˜ì—­:
1. **í•™ìŠµ ì§€ë„**: í•™ìŠµë¶€ì§„, ìˆ™ì œ ê´€ë¦¬, ì§‘ì¤‘ë ¥, í•™ìŠµë™ê¸°, ì§„ë¡œ íƒìƒ‰
2. **êµìš° ê´€ê³„**: ì¹œêµ¬ ê´€ê³„, ë”°ëŒë¦¼, ê°ˆë“± í•´ê²°, ì‚¬íšŒì„± ë°œë‹¬
3. **í–‰ë™ ë¬¸ì œ**: ê·œì¹™ ì¤€ìˆ˜, ì£¼ì˜ ì‚°ë§Œ, ê³µê²©ì„±, ë°˜í•­ì  í–‰ë™
4. **ì •ì„œì  ì§€ì›**: ë¶ˆì•ˆ, ìš°ìš¸, ìŠ¤íŠ¸ë ˆìŠ¤, ìì¡´ê°, ì •ì„œ ì¡°ì ˆ
5. **í•™ë¶€ëª¨ ìƒë‹´**: ê°€ì •-í•™êµ ì—°ê³„, ì–‘ìœ¡ ë°©ì‹, ì†Œí†µ ì „ëµ
6. **í•™ê¸‰ ìš´ì˜**: ìƒí™œ ì§€ë„, í™˜ê²½ ì¡°ì„±, ë¬¸ì œ í•´ê²°

## ì‘ë‹µ êµ¬ì¡°:
1. ìƒí™© ì´í•´ ë° ê³µê°
2. ë¬¸ì œ ë¶„ì„ (ê°€ëŠ¥í•œ ì›ì¸)
3. êµ¬ì²´ì  í•´ê²° ë°©ì•ˆ (ë‹¨ê³„ë³„)
4. ì¶”ê°€ ê³ ë ¤ì‚¬í•­ ë° ì£¼ì˜ì 
5. í•„ìš”ì‹œ ì „ë¬¸ê¸°ê´€ ì—°ê³„ ì•ˆë‚´

í•­ìƒ ì´ˆë“±í•™ìƒì˜ ë°œë‹¬ íŠ¹ì„±ê³¼ ê°œë³„ ì°¨ì´ë¥¼ ê³ ë ¤í•˜ì—¬ ë‹µë³€í•´ì£¼ì„¸ìš”.
"""
    
    def _create_context_from_search_results(self, search_results: List[Dict[str, Any]]) -> str:
        """ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì»¨í…ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
        if not search_results:
            return "ê´€ë ¨ ìƒë‹´ ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤."
        
        context_parts = []
        context_parts.append("=== ìœ ì‚¬í•œ ê³¼ê±° ìƒë‹´ ê¸°ë¡ ===")
        
        for i, result in enumerate(search_results[:5], 1):  # ìµœëŒ€ 5ê°œê¹Œì§€ë§Œ ì‚¬ìš©
            similarity = result.get('similarity', 0)
            title = result.get('title', 'ì œëª© ì—†ìŒ')
            student_query = result.get('student_query', '')
            counselor_answer = result.get('counselor_answer', '')
            date = result.get('date', '')
            worry_tags = result.get('worry_tags', '')
            teacher_name = result.get('teacher_name', '')
            
            # ìœ ì‚¬ë„ê°€ ë„ˆë¬´ ë‚®ìœ¼ë©´ ì œì™¸
            if similarity < 0.5:
                continue
                
            context_parts.append(f"""
[ìƒë‹´ê¸°ë¡ #{i}] (ìœ ì‚¬ë„: {similarity:.2f})
- ë‚ ì§œ: {date}
- ë‹´ë‹¹êµì‚¬: {teacher_name}
- ì œëª©: {title}
- ê³ ë¯¼ íƒœê·¸: {worry_tags}
- í•™ìƒ ë¬¸ì˜: {student_query[:300]}{'...' if len(student_query) > 300 else ''}
- ìƒë‹´ ë‹µë³€: {counselor_answer[:400]}{'...' if len(counselor_answer) > 400 else ''}
""")
        
        if len(context_parts) == 1:  # ìœ ì‚¬í•œ ê¸°ë¡ì´ ì—†ëŠ” ê²½ìš°
            context_parts.append("ìœ ì‚¬í•œ ìƒë‹´ ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤. ì¼ë°˜ì ì¸ êµìœ¡í•™ì  ì§€ì‹ì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•˜ê² ìŠµë‹ˆë‹¤.")
        
        context_parts.append("=== ìƒë‹´ ê¸°ë¡ ì¢…ë£Œ ===")
        return "\n".join(context_parts)
    
    async def generate_counseling_response(
        self, 
        user_query: str, 
        search_results: Optional[List[Dict[str, Any]]] = None,
        conversation_history: Optional[List[Dict[str, str]]] = None
    ) -> Dict[str, Any]:
        """
        ìƒë‹´ ì§ˆë¬¸ì— ëŒ€í•œ ì‘ë‹µ ìƒì„±
        
        Args:
            user_query: ì‚¬ìš©ì ì§ˆë¬¸
            search_results: Milvus ê²€ìƒ‰ ê²°ê³¼
            conversation_history: ëŒ€í™” íˆìŠ¤í† ë¦¬ [{"role": "user/assistant", "content": "..."}]
        
        Returns:
            ì‘ë‹µ ë”•ì…”ë„ˆë¦¬
        """
        async def _generate_response():
            # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            system_prompt = self._create_system_prompt()
            
            # ì»¨í…ìŠ¤íŠ¸ ìƒì„± (ê²€ìƒ‰ ê²°ê³¼ í™œìš©)
            context = ""
            if search_results:
                context = self._create_context_from_search_results(search_results)
            
            # í˜„ì¬ ì‹œê°„ ì •ë³´ ì¶”ê°€
            current_time = datetime.now().strftime("%Yë…„ %mì›” %dì¼ %Hì‹œ %Më¶„")
            
            # ì „ì²´ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            full_prompt = f"""{system_prompt}

{context}

[í˜„ì¬ ì‹œê°„: {current_time}]

í˜„ì¬ ìƒë‹´ ìš”ì²­:
{user_query}

ìœ„ì˜ ê´€ë ¨ ìƒë‹´ ê¸°ë¡ì„ ì°¸ê³ í•˜ì—¬, ë‹¤ìŒê³¼ ê°™ì´ êµ¬ì¡°í™”ëœ ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”:

1. **ìƒí™© ì´í•´**: í˜„ì¬ ìƒí™©ì— ëŒ€í•œ ì´í•´ì™€ ê³µê° í‘œí˜„
2. **ë¬¸ì œ ë¶„ì„**: ê°€ëŠ¥í•œ ì›ì¸ê³¼ ë°°ê²½ ìš”ì¸ë“¤
3. **í•´ê²° ë°©ì•ˆ**: êµ¬ì²´ì ì´ê³  ì‹¤í–‰ ê°€ëŠ¥í•œ ë‹¨ê³„ë³„ í•´ê²°ì±…
4. **ì¶”ê°€ ê³ ë ¤ì‚¬í•­**: ì£¼ì˜í•  ì ì´ë‚˜ ì¥ê¸°ì  ê´€ì ì—ì„œì˜ ì¡°ì–¸
5. **í•„ìš”ì‹œ ì—°ê³„**: ì „ë¬¸ê¸°ê´€ì´ë‚˜ ì¶”ê°€ ì§€ì›ì´ í•„ìš”í•œ ê²½ìš° ì•ˆë‚´

ì „ë¬¸ì ì´ë©´ì„œë„ ì‹¤ë¬´ì—ì„œ ë°”ë¡œ ì ìš©í•  ìˆ˜ ìˆëŠ” ì¡°ì–¸ì„ ë¶€íƒë“œë¦½ë‹ˆë‹¤."""

            # ëŒ€í™” íˆìŠ¤í† ë¦¬ê°€ ìˆëŠ” ê²½ìš° chat ì„¸ì…˜ ì‚¬ìš©
            if conversation_history:
                # Reconstruct the chat history using LangChain message objects
                langchain_history = []
                for msg in conversation_history[-10:]:
                    if msg["role"] == "user":
                        langchain_history.append(HumanMessage(content=msg["content"]))
                    elif msg["role"] == "assistant":
                        langchain_history.append(AIMessage(content=msg["content"]))

                # Append the current prompt to the history
                langchain_history.append(HumanMessage(content=full_prompt))

                # Invoke the model with the complete history
                # The model will use the history as context for the new prompt
                response = await self.model.ainvoke(langchain_history)

                # Access the generated text
                return response.content
            else:
                response = await self.model.ainvoke(
                [HumanMessage(content=full_prompt)],
                config={
                    "temperature": 0.7,
                    "top_p": 0.9,
                    "top_k": 40,
                    "max_output_tokens": 3000,
                    "candidate_count": 1,
                }
            )

            return response.content
        
        try:
            async with self._chat_semaphore:
                response_text = await self._retry_api_call(_generate_response)
                
                return {
                    "status": "success",
                    "response": response_text,
                    "timestamp": datetime.now().isoformat(),
                    "used_context": bool(search_results),
                    "context_count": len(search_results) if search_results else 0,
                    "context_quality": self._assess_context_quality(search_results) if search_results else None
                }
                
        except Exception as e:
            return {
                "status": "error",
                "error": f"Gemini API í˜¸ì¶œ ì‹¤íŒ¨: {str(e)}",
                "timestamp": datetime.now().isoformat()
            }
    
    def _assess_context_quality(self, search_results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ê²€ìƒ‰ ê²°ê³¼ì˜ í’ˆì§ˆì„ í‰ê°€"""
        if not search_results:
            return {"quality": "none", "score": 0}
        
        high_quality_count = sum(1 for r in search_results if r.get('similarity', 0) > 0.8)
        medium_quality_count = sum(1 for r in search_results if 0.6 <= r.get('similarity', 0) <= 0.8)
        
        total_count = len(search_results)
        avg_similarity = sum(r.get('similarity', 0) for r in search_results) / total_count
        
        if avg_similarity > 0.8:
            quality = "excellent"
        elif avg_similarity > 0.6:
            quality = "good"
        elif avg_similarity > 0.4:
            quality = "fair"
        else:
            quality = "poor"
        
        return {
            "quality": quality,
            "average_similarity": round(avg_similarity, 3),
            "high_quality_results": high_quality_count,
            "medium_quality_results": medium_quality_count,
            "total_results": total_count
        }
    
    async def generate_summary(self, conversation_history: List[Dict[str, str]]) -> Dict[str, Any]:
        """ëŒ€í™” ë‚´ìš© ìš”ì•½ ìƒì„±"""
        async def _generate_summary():
            # ëŒ€í™” ë‚´ìš©ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
            conversation_text = "\n".join([
                f"{'ğŸ‘©â€ğŸ« ì„ ìƒë‹˜' if msg['role'] == 'user' else 'ğŸ¤– AI ìƒë‹´ì‚¬'}: {msg['content']}"
                for msg in conversation_history
            ])
            
            summary_prompt = f"""
ë‹¤ìŒì€ ì´ˆë“±í•™êµ ì„ ìƒë‹˜ê³¼ AI ìƒë‹´ì‚¬ì˜ ëŒ€í™” ë‚´ìš©ì…ë‹ˆë‹¤. êµìœ¡ í˜„ì¥ì—ì„œ í™œìš©í•  ìˆ˜ ìˆë„ë¡ ì²´ê³„ì ìœ¼ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”.

ëŒ€í™” ë‚´ìš©:
{conversation_text}

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ìƒë‹´ ìš”ì•½ì„œë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”:

## ğŸ“‹ ìƒë‹´ ìš”ì•½ì„œ

### 1. ìƒë‹´ ê°œìš”
- ìƒë‹´ ì£¼ì œ: 
- ì£¼ìš” ê´€ì‹¬ì‚¬:
- ìƒë‹´ ì‹œì :

### 2. ë¬¸ì œ ìƒí™©
- í˜„ì¬ ìƒí™©:
- ì£¼ìš” ì–´ë ¤ì›€:
- ê´€ë ¨ ìš”ì¸ë“¤:

### 3. ìƒë‹´ ë‚´ìš© ë° ì œì•ˆì‚¬í•­
- ë…¼ì˜ëœ í•´ê²°ì±…:
- êµ¬ì²´ì  ì‹¤í–‰ ë°©ë²•:
- ë‹¨ê¸°/ì¥ê¸° ê³„íš:

### 4. í–¥í›„ ì¡°ì¹˜ì‚¬í•­
- ì¦‰ì‹œ ì‹¤í–‰í•  ì‚¬í•­:
- ê²½ê³¼ ê´€ì°° í¬ì¸íŠ¸:
- ì¶”ê°€ ì§€ì› í•„ìš”ì‚¬í•­:

### 5. ì°¸ê³ ì‚¬í•­
- ì£¼ì˜í•  ì :
- í•™ë¶€ëª¨ ìƒë‹´ í•„ìš”ì„±:
- ì „ë¬¸ê¸°ê´€ ì—°ê³„ í•„ìš”ì„±:

ì‹¤ë¬´ì—ì„œ ë°”ë¡œ ì°¸ê³ í•  ìˆ˜ ìˆë„ë¡ ê°„ê²°í•˜ê³  ëª…í™•í•˜ê²Œ ì •ë¦¬í•´ì£¼ì„¸ìš”.
"""
            
            response = await self.model.ainvoke(
                [HumanMessage(content=summary_prompt)],
                config={
                    "temperature": 0.3,
                    "max_output_tokens": 2000,
                }
            )

            return response.content
        
        try:
            async with self._chat_semaphore:
                summary_text = await self._retry_api_call(_generate_summary)
                
                return {
                    "status": "success",
                    "summary": summary_text,
                    "timestamp": datetime.now().isoformat(),
                    "conversation_length": len(conversation_history)
                }
                
        except Exception as e:
            return {
                "status": "error",
                "error": f"ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {str(e)}",
                "timestamp": datetime.now().isoformat()
            }
    
    async def generate_keywords(self, text: str) -> Dict[str, Any]:
        """í…ìŠ¤íŠ¸ì—ì„œ ê³ ë¯¼ íƒœê·¸/í‚¤ì›Œë“œ ì¶”ì¶œ"""
        async def _generate_keywords():
            keywords_prompt = f"""
ë‹¤ìŒ ìƒë‹´ ë‚´ìš©ì—ì„œ ì£¼ìš” í‚¤ì›Œë“œì™€ ê³ ë¯¼ íƒœê·¸ë¥¼ ì¶”ì¶œí•´ì£¼ì„¸ìš”.

ìƒë‹´ ë‚´ìš©:
{text}

ì´ˆë“±í•™êµ ìƒë‹´ì—ì„œ ìì£¼ ì‚¬ìš©ë˜ëŠ” í‘œì¤€ ì¹´í…Œê³ ë¦¬ë¡œ ë¶„ë¥˜í•˜ì—¬ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ì„¸ìš”:

**í•™ìŠµ ê´€ë ¨**: í•™ìŠµë¶€ì§„, ìˆ™ì œë¯¸ì™„ì„±, ì§‘ì¤‘ë ¥ë¶€ì¡±, í•™ìŠµë™ê¸°ì €í•˜, ì„±ì í•˜ë½, í•™ìŠµìŠµê´€, ë…ì„œë¶€ì§„ ë“±
**êµìš°ê´€ê³„**: ì¹œêµ¬ê´€ê³„, ë”°ëŒë¦¼, ê°ˆë“±, ì‚¬íšŒì„±ë¶€ì¡±, ë¦¬ë”ì‹­, ì†Œê·¹ì„±, ê³µê²©ì„±, í˜‘ë ¥ ë“±  
**í–‰ë™ë¬¸ì œ**: ê·œì¹™ìœ„ë°˜, ì‚°ë§Œí•¨, ì¶©ë™ì„±, ë°˜í•­, ê±°ì§“ë§, ë„ë²½, ì£¼ì˜ì§‘ì¤‘ ë“±
**ì •ì„œë¬¸ì œ**: ë¶ˆì•ˆ, ìš°ìš¸, ìŠ¤íŠ¸ë ˆìŠ¤, ìœ„ì¶•, ìì‹ ê°ë¶€ì¡±, ì™„ë²½ì£¼ì˜, ê°ì •ì¡°ì ˆ ë“±
**ê°€ì •í™˜ê²½**: ê°€ì¡±ê°ˆë“±, ë¶€ëª¨ì´í˜¼, ê²½ì œì ì–´ë ¤ì›€, ë°©ì„, ê³¼ë³´í˜¸, í˜•ì œê°ˆë“± ë“±
**ì‹ ì²´ê±´ê°•**: ì‹ìŠµê´€, ìˆ˜ë©´, ìœ„ìƒ, ì„±ì¥, ì‹œë ¥, ë¹„ë§Œ, í—ˆì•½ì²´ì§ˆ ë“±
**ê¸°íƒ€**: ì§„ë¡œ, íŠ¹ê¸°ì ì„±, ì°½ì˜ì„±, ì˜ˆì²´ëŠ¥, ë´‰ì‚¬í™œë™, ë¦¬ë”ì‹­ ë“±

ê²°ê³¼ë¥¼ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì œì‹œí•´ì£¼ì„¸ìš”:
- ì£¼ìš” ì¹´í…Œê³ ë¦¬: [ì¹´í…Œê³ ë¦¬ëª…]
- í•µì‹¬ í‚¤ì›Œë“œ: [í‚¤ì›Œë“œ1, í‚¤ì›Œë“œ2, í‚¤ì›Œë“œ3] (ìµœëŒ€ 5ê°œ)
- ìƒë‹´ ìš°ì„ ìˆœìœ„: ë†’ìŒ/ì¤‘ê°„/ë‚®ìŒ
- ì „ë¬¸ê¸°ê´€ ì—°ê³„ í•„ìš”ì„±: í•„ìš”/ì„ íƒì /ë¶ˆí•„ìš”
"""
            
            response = await self.model.ainvoke(
                [HumanMessage(content=keywords_prompt)],
                config={
                    "temperature": 0.3,
                    "max_output_tokens": 512,
                }
            )

            return response.content
        
        try:
            async with self._chat_semaphore:
                keywords_text = await self._retry_api_call(_generate_keywords)
                
                return {
                    "status": "success",
                    "keywords": keywords_text,
                    "timestamp": datetime.now().isoformat(),
                    "text_length": len(text)
                }
                
        except Exception as e:
            return {
                "status": "error",
                "error": f"í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}",
                "timestamp": datetime.now().isoformat()
            }

    async def generate_counseling_plan(self, student_info: Dict[str, Any]) -> Dict[str, Any]:
        """í•™ìƒ ì •ë³´ ê¸°ë°˜ ê°œë³„ ìƒë‹´ ê³„íš ìˆ˜ë¦½"""
        async def _generate_plan():
            plan_prompt = f"""
ë‹¤ìŒ í•™ìƒ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê°œë³„ ìƒë‹´ ê³„íšì„ ìˆ˜ë¦½í•´ì£¼ì„¸ìš”.

í•™ìƒ ì •ë³´:
{json.dumps(student_info, ensure_ascii=False, indent=2)}

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ê°œë³„ ìƒë‹´ ê³„íšì„œë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”:

## ğŸ“ ê°œë³„ ìƒë‹´ ê³„íšì„œ

### 1. í•™ìƒ í˜„í™© ë¶„ì„
- ì£¼ìš” ê°•ì :
- ê°œì„ ì´ í•„ìš”í•œ ì˜ì—­:
- ê´€ì°° í¬ì¸íŠ¸:

### 2. ìƒë‹´ ëª©í‘œ
- ë‹¨ê¸° ëª©í‘œ (1ê°œì›”):
- ì¤‘ê¸° ëª©í‘œ (1í•™ê¸°):
- ì¥ê¸° ëª©í‘œ (1ë…„):

### 3. ìƒë‹´ ì „ëµ
- ì ‘ê·¼ ë°©ë²•:
- ìƒë‹´ ê¸°ë²•:
- ë™ê¸° ë¶€ì—¬ ë°©ë²•:

### 4. ì‹¤í–‰ ê³„íš
- ìƒë‹´ ì£¼ê¸°: 
- í™œë™ ê³„íš:
- í‰ê°€ ë°©ë²•:

### 5. ì§€ì› ì²´ê³„
- í•™ê¸‰ ë‚´ ì§€ì›:
- ê°€ì • ì—°ê³„ ë°©ì•ˆ:
- ì „ë¬¸ê¸°ê´€ í™œìš©:

ì´ˆë“±í•™ìƒì˜ ë°œë‹¬ íŠ¹ì„±ì„ ê³ ë ¤í•˜ì—¬ ì‹¤í˜„ ê°€ëŠ¥í•œ ê³„íšì„ ìˆ˜ë¦½í•´ì£¼ì„¸ìš”.
"""
            
            response = await self.model.ainvoke(
                [HumanMessage(content=plan_prompt)],
                config={
                    "temperature": 0.5,
                    "max_output_tokens": 2500,
                }
            )

            return response.content
        
        try:
            async with self._chat_semaphore:
                plan_text = await self._retry_api_call(_generate_plan)
                
                return {
                    "status": "success",
                    "counseling_plan": plan_text,
                    "timestamp": datetime.now().isoformat()
                }
                
        except Exception as e:
            return {
                "status": "error",
                "error": f"ìƒë‹´ ê³„íš ìˆ˜ë¦½ ì‹¤íŒ¨: {str(e)}",
                "timestamp": datetime.now().isoformat()
            }

# ì „ì—­ ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤
gemini_service = GeminiChatService()