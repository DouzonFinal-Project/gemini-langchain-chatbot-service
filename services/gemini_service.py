# services/gemini_service.py

import os
import asyncio
import logging
from zoneinfo import ZoneInfo
import itertools
import json
from typing import List, Dict, Any, Optional, AsyncGenerator
from datetime import datetime

from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.messages import (
    HumanMessage,   # ì‚¬ëŒì´ ë³´ë‚¸ ë©”ì„¸ì§€
    AIMessage,      # AIì˜ ì‘ë‹µ ë©”ì„¸ì§€
    SystemMessage,  # ì‹œìŠ¤í…œì˜ ì§€ì‹œ ë©”ì„¸ì§€
    ToolMessage,    # ë„êµ¬ì™€ ê´€ë ¨ëœ ë©”ì„¸ì§€
    trim_messages   # ë©”ì„¸ì§€ ë‹¤ë“¬ê¸° í•¨ìˆ˜
    )
from langchain.schema import BaseMessage
from dotenv import load_dotenv

# í™˜ê²½ ì„¤ì • ë¡œë“œ
load_dotenv()
logger = logging.getLogger(__name__)

GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
GEMINI_MODEL_CHAT = os.getenv("GEMINI_MODEL_CHAT", "gemini-2.5-flash-lite")

if not GEMINI_API_KEY:
    raise ValueError("GEMINI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

# ìƒì„±ì í•¨ìˆ˜ â€” ì¬ì‚¬ìš© ê°€ëŠ¥í•œ LLM ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜
def get_llm(disable_streaming: bool = False) -> ChatGoogleGenerativeAI:
    api_key = os.getenv("GEMINI_API_KEY") or os.getenv("GOOGLE_API_KEY")
    llm = ChatGoogleGenerativeAI(
        model="gemini-2.5-flash-lite",
        google_api_key=api_key,
        temperature=0.7,
        disable_streaming=disable_streaming
    )
    return llm

# ìŠ¤íŠ¸ë¦¬ë° ìƒì„±: messagesë¥¼ ë°›ì•„ async generatorë¡œ chunkë“¤ì„ yield
async def stream_generate(messages: List[BaseMessage]) -> AsyncGenerator[str, None]:
    """
    ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸(SystemMessage, HumanMessage ë“±)ë¥¼ ë°›ì•„
    LLMì˜ astream()ì„ í†µí•´ ë¶€ë¶„ ê²°ê³¼ ë¬¸ìì—´ì„ ìˆœì°¨ì ìœ¼ë¡œ yield í•©ë‹ˆë‹¤.
    """
    llm = get_llm(disable_streaming=False)
    generated = ""
    async for chunk in llm.astream(messages):
        # chunk ê°ì²´ í˜•ì‹ì€ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë²„ì „ì— ë”°ë¼ ë‹¤ë¦„ â€” ì•ˆì „í•˜ê²Œ ì ‘ê·¼
        content = getattr(chunk, "content", None) or getattr(chunk, "text", None)
        if content:
            generated += content
            yield content
    # ì™„ë£Œ ì‹œ ì•„ë¬´ ê²ƒë„ ë°˜í™˜í•˜ì§€ ì•ŠìŒ; í˜¸ì¶œìëŠ” ì™„ì„±ëœ í…ìŠ¤íŠ¸ ê¸¸ì´ ë“± ì²˜ë¦¬ ê°€ëŠ¥

# ë¹„ìŠ¤íŠ¸ë¦¬ë° ìƒì„±: í•œ ë²ˆì— ì‘ë‹µ ê°ì²´ ë°˜í™˜ (ë¹„ë™ê¸°)
async def generate(messages: List[BaseMessage]) -> dict:
    llm = get_llm(disable_streaming=True)
    resp = await llm.ainvoke(messages)
    content = getattr(resp, "content", "") or getattr(resp, "text", "")
    return {
        "content": content,
        "length": len(content),
        "generated_at": datetime.now().isoformat()
    }
class GeminiChatService:
    """LangChainì„ ì‚¬ìš©í•œ Gemini ì±—ë´‡ ì„œë¹„ìŠ¤"""
    
    def __init__(self):
        self.model = ChatGoogleGenerativeAI(
            model=GEMINI_MODEL_CHAT,
            google_api_key=GEMINI_API_KEY
        )
        self._chat_semaphore = asyncio.Semaphore(3)  # ë™ì‹œ ìš”ì²­ ì œí•œ
        self._max_retries = 3
        self._retry_delay = 1.0
    
    async def _run_blocking(self, fn, *args, **kwargs):
        """ë¹„ë™ê¸°ë¡œ ë¸”ë¡œí‚¹ í•¨ìˆ˜ ì‹¤í–‰"""
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(None, lambda: fn(*args, **kwargs))
    
    async def _retry_api_call(self, api_func, *args, **kwargs):
        """ì¬ì‹œë„ ë¡œì§ì´ í¬í•¨ëœ API í˜¸ì¶œ"""
        last_error = None
        
        for attempt in range(self._max_retries):
            try:
                return await api_func(*args, **kwargs)
            except Exception as e:
                last_error = e
                if attempt < self._max_retries - 1:
                    wait_time = self._retry_delay * (2 ** attempt)
                    print(f"API í˜¸ì¶œ ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}/{self._max_retries}), {wait_time}ì´ˆ í›„ ì¬ì‹œë„: {e}")
                    await asyncio.sleep(wait_time)
                else:
                    print(f"API í˜¸ì¶œ ìµœì¢… ì‹¤íŒ¨: {e}")
        
        raise last_error
    
    def _create_system_prompt(self) -> str:
        """ì´ˆë“±í•™êµ 6í•™ë…„ ë‹´ì„ì„ ìƒë‹˜ì„ ìœ„í•œ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸"""
        return """
You are an elementary school counselor assistant.

You are expected to answer the counselor's questions with pedagogical evidence.

You should follow these guidelines:
1. Provide professional yet understandable explanations.
2. Provide specific and actionable advice.
3. Prioritize practical solutions for elementary school students.

Main counseling areas:
1. Peer relationships: Friendships, bullying, conflict resolution, social development.
2. Parent counseling: Home-school connections, parenting styles, communication strategies.
3. Behavioral issues: Rule compliance, distractibility, aggression, and oppositional behavior.

Response structure:

As an elementary school counselor assistant, explain your perspective step-by-step.

Avoid short answers and provide context-sensitive responses.

Please provide all responses in Korean.

At the end of your answer, provide the user with two suggested questions.

Don't introduce yourself or say hello unless the user asks you to.
"""
    
    def _create_context_from_search_results(self, search_results: List[Dict[str, Any]]) -> str:
        """ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì»¨í…ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
        if not search_results:
            return "ê´€ë ¨ ìƒë‹´ ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤."
        
        context_parts = []
        context_parts.append("=== ìœ ì‚¬í•œ ê³¼ê±° ìƒë‹´ ê¸°ë¡ ===")
        
        for i, result in enumerate(search_results[:5], 1):  # ìµœëŒ€ 5ê°œê¹Œì§€ë§Œ ì‚¬ìš©
            similarity = result.get('similarity', 0)
            title = result.get('title', 'ì œëª© ì—†ìŒ')
            student_query = result.get('student_query', '')
            counselor_answer = result.get('counselor_answer', '')
            date = result.get('date', '')
            worry_tags = result.get('worry_tags', '')
            teacher_name = result.get('teacher_name', '')
            
            # ìœ ì‚¬ë„ê°€ ë„ˆë¬´ ë‚®ìœ¼ë©´ ì œì™¸
            if similarity < 0.1:
                continue
                
            context_parts.append(f"""
[ìƒë‹´ê¸°ë¡ #{i}] (ìœ ì‚¬ë„: {similarity:.2f})
- ë‚ ì§œ: {date}
- ë‹´ë‹¹êµì‚¬: {teacher_name}
- ì œëª©: {title}
- ê³ ë¯¼ íƒœê·¸: {worry_tags}
- í•™ìƒ ë¬¸ì˜: {student_query[:300]}{'...' if len(student_query) > 300 else ''}
- ìƒë‹´ ë‹µë³€: {counselor_answer[:400]}{'...' if len(counselor_answer) > 400 else ''}
""")
        
        if len(context_parts) == 1:  # ìœ ì‚¬í•œ ê¸°ë¡ì´ ì—†ëŠ” ê²½ìš°
            context_parts.append("ìœ ì‚¬í•œ ìƒë‹´ ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤. ì¼ë°˜ì ì¸ êµìœ¡í•™ì  ì§€ì‹ì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•˜ê² ìŠµë‹ˆë‹¤.")
        
        context_parts.append("=== ìƒë‹´ ê¸°ë¡ ì¢…ë£Œ ===")
        return "\n".join(context_parts)
    
    async def generate_counseling_response(
        self, 
        user_query: str, 
        search_results: Optional[List[Dict[str, Any]]] = None,
        conversation_history: Optional[List[Dict[str, str]]] = None
    ) -> Dict[str, Any]:
        """ê°œì„ ëœ ìƒë‹´ ì‘ë‹µ ìƒì„±"""
        
        async def _generate_response():
            system_prompt = self._create_system_prompt()
            
            # ì»¨í…ìŠ¤íŠ¸ ìƒì„± - ë” ìƒì„¸í•œ ë¡œê¹…
            context = ""
            if search_results:
                print(f"RAG ì»¨í…ìŠ¤íŠ¸ ìƒì„± ì¤‘... ê²€ìƒ‰ ê²°ê³¼ {len(search_results)}ê°œ")
                context = self._create_context_from_search_results(search_results)
                print(f"ìƒì„±ëœ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´: {len(context)} ë¬¸ì")
            else:
                print("RAG ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ì–´ ê¸°ë³¸ ëª¨ë“œë¡œ ì‘ë‹µ ìƒì„±")
                context = "ê´€ë ¨ ìƒë‹´ ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤. ì¼ë°˜ì ì¸ êµìœ¡í•™ì  ì§€ì‹ì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•˜ê² ìŠµë‹ˆë‹¤."
            
            current_time = datetime.now().strftime("%Yë…„ %mì›” %dì¼ %Hì‹œ %Më¶„")
            
            # í”„ë¡¬í”„íŠ¸ì— RAG ì‚¬ìš© ì—¬ë¶€ ëª…ì‹œ
            rag_indicator = "[RAG í™œì„±í™”]" if search_results else "[ê¸°ë³¸ ëª¨ë“œ]"
            
            full_prompt = f"""{system_prompt}

{rag_indicator}

{context}

[í˜„ì¬ ì‹œê°„: {current_time}]

í˜„ì¬ ìƒë‹´ ìš”ì²­:
{user_query}

ìœ„ì˜ {"ê´€ë ¨ ìƒë‹´ ê¸°ë¡ì„ ì°¸ê³ í•˜ì—¬" if search_results else "êµìœ¡í•™ì  ì§€ì‹ì„ ë°”íƒ•ìœ¼ë¡œ"}, ë‹¤ìŒê³¼ ê°™ì´ ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”:

{search_results}ì˜ ë‚´ìš©ê³¼ ë§¥ë½ì„ íŒŒì•…í•˜ê³  í•™ìƒì„ ìœ„í•œ ë‹µë³€ ì œê³µì„ í•´ì£¼ì„¸ìš”

ì „ë¬¸ì ì´ë©´ì„œë„ ì‹¤ë¬´ì—ì„œ ë°”ë¡œ ì ìš©í•  ìˆ˜ ìˆëŠ” ì¡°ì–¸ì„ ë¶€íƒë“œë¦½ë‹ˆë‹¤.
(ì‘ë‹µ ê¸¸ì´: 1000ì ì´ë‚´)"""

            # ëŒ€í™” íˆìŠ¤í† ë¦¬ ì²˜ë¦¬
            if conversation_history:
                langchain_history = []
                for msg in conversation_history[-10:]:
                    if msg["role"] == "user":
                        langchain_history.append(HumanMessage(content=msg["content"]))
                    elif msg["role"] == "assistant":
                        langchain_history.append(AIMessage(content=msg["content"]))
                
                langchain_history.append(HumanMessage(content=full_prompt))
                response = await self.model.ainvoke(langchain_history)
                return response.content
            else:
                response = await self.model.ainvoke([HumanMessage(content=full_prompt)])
                return response.content
        
        try:
            async with self._chat_semaphore:
                response_text = await self._retry_api_call(_generate_response)
                
                # ì‘ë‹µ í’ˆì§ˆ í‰ê°€ ì¶”ê°€
                response_quality = self._assess_response_quality(response_text, search_results)
                
                return {
                    "status": "success",
                    "response": response_text,
                    "timestamp": datetime.now().isoformat(),
                    "used_context": bool(search_results),
                    "context_count": len(search_results) if search_results else 0,
                    "context_quality": self._assess_context_quality(search_results) if search_results else None,
                    "response_quality": response_quality
                }
                
        except Exception as e:
            print(f"ìƒë‹´ ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {str(e)}")
            import traceback
            traceback.print_exc()
            return {
                "status": "error",
                "error": f"Gemini API í˜¸ì¶œ ì‹¤íŒ¨: {str(e)}",
                "timestamp": datetime.now().isoformat()
            }

    def _assess_response_quality(self, response_text: str, search_results: Optional[List]) -> Dict[str, Any]:
        """ì‘ë‹µ í’ˆì§ˆ í‰ê°€"""
        return {
            "length": len(response_text),
            "has_structure": "**" in response_text or "##" in response_text,
            "used_rag_context": bool(search_results),
            "estimated_sections": response_text.count("**") // 2
        }
    
    def _assess_context_quality(self, search_results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ê²€ìƒ‰ ê²°ê³¼ì˜ í’ˆì§ˆì„ í‰ê°€"""
        if not search_results:
            return {"quality": "none", "score": 0}
        
        high_quality_count = sum(1 for r in search_results if r.get('similarity', 0) > 0.8)
        medium_quality_count = sum(1 for r in search_results if 0.6 <= r.get('similarity', 0) <= 0.8)
        
        total_count = len(search_results)
        avg_similarity = sum(r.get('similarity', 0) for r in search_results) / total_count
        
        if avg_similarity > 0.8:
            quality = "excellent"
        elif avg_similarity > 0.6:
            quality = "good"
        elif avg_similarity > 0.4:
            quality = "fair"
        else:
            quality = "poor"
        
        return {
            "quality": quality,
            "average_similarity": round(avg_similarity, 3),
            "high_quality_results": high_quality_count,
            "medium_quality_results": medium_quality_count,
            "total_results": total_count
        }
    
    async def generate_summary(self, conversation_history: List[Dict[str, str]]) -> Dict[str, Any]:
        """ëŒ€í™” ë‚´ìš© ìš”ì•½ ìƒì„±"""
        async def _generate_summary():
            # ëŒ€í™” ë‚´ìš©ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
            conversation_text = "\n".join([
                f"{'ğŸ‘©â€ğŸ« ì„ ìƒë‹˜' if msg['role'] == 'user' else 'ğŸ¤– AI ìƒë‹´ì‚¬'}: {msg['content']}"
                for msg in conversation_history
            ])
            
            summary_prompt = f"""
ë‹¤ìŒì€ ì´ˆë“±í•™êµ ì„ ìƒë‹˜ê³¼ AI ìƒë‹´ì‚¬ì˜ ëŒ€í™” ë‚´ìš©ì…ë‹ˆë‹¤. êµìœ¡ í˜„ì¥ì—ì„œ í™œìš©í•  ìˆ˜ ìˆë„ë¡ ì²´ê³„ì ìœ¼ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”.

ëŒ€í™” ë‚´ìš©:
{conversation_text}

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ìƒë‹´ ìš”ì•½ì„œë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”:

## ğŸ“‹ ìƒë‹´ ìš”ì•½ì„œ

### 1. ìƒë‹´ ê°œìš”
- ìƒë‹´ ì£¼ì œ: 
- ì£¼ìš” ê´€ì‹¬ì‚¬:
- ìƒë‹´ ì‹œì :

### 2. ë¬¸ì œ ìƒí™©
- í˜„ì¬ ìƒí™©:
- ì£¼ìš” ì–´ë ¤ì›€:
- ê´€ë ¨ ìš”ì¸ë“¤:

### 3. ìƒë‹´ ë‚´ìš© ë° ì œì•ˆì‚¬í•­
- ë…¼ì˜ëœ í•´ê²°ì±…:
- êµ¬ì²´ì  ì‹¤í–‰ ë°©ë²•:
- ë‹¨ê¸°/ì¥ê¸° ê³„íš:

### 4. í–¥í›„ ì¡°ì¹˜ì‚¬í•­
- ì¦‰ì‹œ ì‹¤í–‰í•  ì‚¬í•­:
- ê²½ê³¼ ê´€ì°° í¬ì¸íŠ¸:
- ì¶”ê°€ ì§€ì› í•„ìš”ì‚¬í•­:

### 5. ì°¸ê³ ì‚¬í•­
- ì£¼ì˜í•  ì :
- í•™ë¶€ëª¨ ìƒë‹´ í•„ìš”ì„±:
- ì „ë¬¸ê¸°ê´€ ì—°ê³„ í•„ìš”ì„±:

ì‹¤ë¬´ì—ì„œ ë°”ë¡œ ì°¸ê³ í•  ìˆ˜ ìˆë„ë¡ ê°„ê²°í•˜ê³  ëª…í™•í•˜ê²Œ ì •ë¦¬í•´ì£¼ì„¸ìš”.
"""
            
            response = await self.model.ainvoke(
                [HumanMessage(content=summary_prompt)],
                config={
                    "temperature": 0.3,
                    "max_output_tokens": 2000,
                }
            )

            return response.content
        
        try:
            async with self._chat_semaphore:
                summary_text = await self._retry_api_call(_generate_summary)
                
                return {
                    "status": "success",
                    "summary": summary_text,
                    "timestamp": datetime.now().isoformat(),
                    "conversation_length": len(conversation_history)
                }
                
        except Exception as e:
            return {
                "status": "error",
                "error": f"ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {str(e)}",
                "timestamp": datetime.now().isoformat()
            }
    
    async def generate_keywords(self, text: str) -> Dict[str, Any]:
        """í…ìŠ¤íŠ¸ì—ì„œ ê³ ë¯¼ íƒœê·¸/í‚¤ì›Œë“œ ì¶”ì¶œ"""
        async def _generate_keywords():
            keywords_prompt = f"""
ë‹¤ìŒ ìƒë‹´ ë‚´ìš©ì—ì„œ ì£¼ìš” í‚¤ì›Œë“œì™€ ê³ ë¯¼ íƒœê·¸ë¥¼ ì¶”ì¶œí•´ì£¼ì„¸ìš”.

ìƒë‹´ ë‚´ìš©:
{text}

ì´ˆë“±í•™êµ ìƒë‹´ì—ì„œ ìì£¼ ì‚¬ìš©ë˜ëŠ” í‘œì¤€ ì¹´í…Œê³ ë¦¬ë¡œ ë¶„ë¥˜í•˜ì—¬ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ì„¸ìš”:

**í•™ìŠµ ê´€ë ¨**: í•™ìŠµë¶€ì§„, ìˆ™ì œë¯¸ì™„ì„±, ì§‘ì¤‘ë ¥ë¶€ì¡±, í•™ìŠµë™ê¸°ì €í•˜, ì„±ì í•˜ë½, í•™ìŠµìŠµê´€, ë…ì„œë¶€ì§„ ë“±
**êµìš°ê´€ê³„**: ì¹œêµ¬ê´€ê³„, ë”°ëŒë¦¼, ê°ˆë“±, ì‚¬íšŒì„±ë¶€ì¡±, ë¦¬ë”ì‹­, ì†Œê·¹ì„±, ê³µê²©ì„±, í˜‘ë ¥ ë“±  
**í–‰ë™ë¬¸ì œ**: ê·œì¹™ìœ„ë°˜, ì‚°ë§Œí•¨, ì¶©ë™ì„±, ë°˜í•­, ê±°ì§“ë§, ë„ë²½, ì£¼ì˜ì§‘ì¤‘ ë“±
**ì •ì„œë¬¸ì œ**: ë¶ˆì•ˆ, ìš°ìš¸, ìŠ¤íŠ¸ë ˆìŠ¤, ìœ„ì¶•, ìì‹ ê°ë¶€ì¡±, ì™„ë²½ì£¼ì˜, ê°ì •ì¡°ì ˆ ë“±
**ê°€ì •í™˜ê²½**: ê°€ì¡±ê°ˆë“±, ë¶€ëª¨ì´í˜¼, ê²½ì œì ì–´ë ¤ì›€, ë°©ì„, ê³¼ë³´í˜¸, í˜•ì œê°ˆë“± ë“±
**ì‹ ì²´ê±´ê°•**: ì‹ìŠµê´€, ìˆ˜ë©´, ìœ„ìƒ, ì„±ì¥, ì‹œë ¥, ë¹„ë§Œ, í—ˆì•½ì²´ì§ˆ ë“±
**ê¸°íƒ€**: ì§„ë¡œ, íŠ¹ê¸°ì ì„±, ì°½ì˜ì„±, ì˜ˆì²´ëŠ¥, ë´‰ì‚¬í™œë™, ë¦¬ë”ì‹­ ë“±

ê²°ê³¼ë¥¼ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì œì‹œí•´ì£¼ì„¸ìš”:
- ì£¼ìš” ì¹´í…Œê³ ë¦¬: [ì¹´í…Œê³ ë¦¬ëª…]
- í•µì‹¬ í‚¤ì›Œë“œ: [í‚¤ì›Œë“œ1, í‚¤ì›Œë“œ2, í‚¤ì›Œë“œ3] (ìµœëŒ€ 5ê°œ)
- ìƒë‹´ ìš°ì„ ìˆœìœ„: ë†’ìŒ/ì¤‘ê°„/ë‚®ìŒ
- ì „ë¬¸ê¸°ê´€ ì—°ê³„ í•„ìš”ì„±: í•„ìš”/ì„ íƒì /ë¶ˆí•„ìš”
"""
            
            response = await self.model.ainvoke(
                [HumanMessage(content=keywords_prompt)],
                config={
                    "temperature": 0.3,
                    "max_output_tokens": 512,
                }
            )

            return response.content
        
        try:
            async with self._chat_semaphore:
                keywords_text = await self._retry_api_call(_generate_keywords)
                
                return {
                    "status": "success",
                    "keywords": keywords_text,
                    "timestamp": datetime.now().isoformat(),
                    "text_length": len(text)
                }
                
        except Exception as e:
            return {
                "status": "error",
                "error": f"í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}",
                "timestamp": datetime.now().isoformat()
            }

    async def generate_counseling_plan(self, student_info: Dict[str, Any]) -> Dict[str, Any]:
        """í•™ìƒ ì •ë³´ ê¸°ë°˜ ê°œë³„ ìƒë‹´ ê³„íš ìˆ˜ë¦½"""
        async def _generate_plan():
            plan_prompt = f"""
ë‹¤ìŒ í•™ìƒ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê°œë³„ ìƒë‹´ ê³„íšì„ ìˆ˜ë¦½í•´ì£¼ì„¸ìš”.

í•™ìƒ ì •ë³´:
{json.dumps(student_info, ensure_ascii=False, indent=2)}

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ê°œë³„ ìƒë‹´ ê³„íšì„œë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”:

## ğŸ“ ê°œë³„ ìƒë‹´ ê³„íšì„œ

### 1. í•™ìƒ í˜„í™© ë¶„ì„
- ì£¼ìš” ê°•ì :
- ê°œì„ ì´ í•„ìš”í•œ ì˜ì—­:
- ê´€ì°° í¬ì¸íŠ¸:

### 2. ìƒë‹´ ëª©í‘œ
- ë‹¨ê¸° ëª©í‘œ (1ê°œì›”):
- ì¤‘ê¸° ëª©í‘œ (1í•™ê¸°):
- ì¥ê¸° ëª©í‘œ (1ë…„):

### 3. ìƒë‹´ ì „ëµ
- ì ‘ê·¼ ë°©ë²•:
- ìƒë‹´ ê¸°ë²•:
- ë™ê¸° ë¶€ì—¬ ë°©ë²•:

### 4. ì‹¤í–‰ ê³„íš
- ìƒë‹´ ì£¼ê¸°: 
- í™œë™ ê³„íš:
- í‰ê°€ ë°©ë²•:

### 5. ì§€ì› ì²´ê³„
- í•™ê¸‰ ë‚´ ì§€ì›:
- ê°€ì • ì—°ê³„ ë°©ì•ˆ:
- ì „ë¬¸ê¸°ê´€ í™œìš©:

ì´ˆë“±í•™ìƒì˜ ë°œë‹¬ íŠ¹ì„±ì„ ê³ ë ¤í•˜ì—¬ ì‹¤í˜„ ê°€ëŠ¥í•œ ê³„íšì„ ìˆ˜ë¦½í•´ì£¼ì„¸ìš”.
"""
            
            response = await self.model.ainvoke(
                [HumanMessage(content=plan_prompt)],
                config={
                    "temperature": 0.5,
                    "max_output_tokens": 2500,
                }
            )

            return response.content
        
        try:
            async with self._chat_semaphore:
                plan_text = await self._retry_api_call(_generate_plan)
                
                return {
                    "status": "success",
                    "counseling_plan": plan_text,
                    "timestamp": datetime.now().isoformat()
                }
                
        except Exception as e:
            return {
                "status": "error",
                "error": f"ìƒë‹´ ê³„íš ìˆ˜ë¦½ ì‹¤íŒ¨: {str(e)}",
                "timestamp": datetime.now().isoformat()
            }

# ì „ì—­ ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤
gemini_service = GeminiChatService()